{
    catalog: "spark_catalog",
    expire_snapshot: {
        // Number of ancestor snapshots to preserve regardless of `older_than`
        // DEFAULT: 1
        // retain_last: 1
    },
    rewrite_data_files: {
        options: {
            // The minimum number of files that need to be in a file group for it to be considered for compaction. Defaults to 5
            "min-input-files": 2,

            // The output file size that this rewrite strategy will attempt to generate when rewriting files.
            // Defaults to 512MB (536870912 bytes)
            // "target-file-size-bytes": 536870912,

            // The entire rewrite operation is broken down into pieces based on partitioning and within partitions based on size into groups. 
            // These sub-units of the rewrite are referred to as file groups. 
            // The largest amount of data that should be compacted in a single group is controlled by MAX_FILE_GROUP_SIZE_BYTES. 
            // This helps with breaking down the rewriting of very large partitions which may not be rewritable otherwise due to the resource constraints of the cluster. 
            // "max-file-group-size-bytes" // default is 1024L * 1024L * 1024L * 100L = 100 GB
        } 
    },
    rewrite_manifests: {
        // Set to false to avoid memory issues on executors
        // use_caching: true
    }
}